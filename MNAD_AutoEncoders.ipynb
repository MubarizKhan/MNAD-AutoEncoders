{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNAD-AutoEncoders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbKtDyCyW0NK",
        "outputId": "95700dbb-2c21-4a43-ebc9-80475fe7ba9e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NHxFJUbavVO"
      },
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, Input,Dense,Flatten,Dropout,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
        ")\n",
        "\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzlhnOnXZH0u"
      },
      "source": [
        "\n",
        "# filename1 = '/content/drive/MyDrive/MNAD-FYP/MADBERT/300kMNAD.h5'\n",
        "# df = pd.read_hdf(filename1)\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbsGavrdZHyG"
      },
      "source": [
        "# f = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/all_data.csv'\n",
        "f = filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/pure_udata.h5'\n",
        "df = pd.read_hdf(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "z5YBVsiOZHvF",
        "outputId": "5c5dc126-f4b3-49a7-903d-5ec6ad2170c2"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>179</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>127</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   Destination Port   Flow Duration  ...   Idle Max   Idle Min   Label\n",
              "0           0                213               0  ...          0          0  BENIGN\n",
              "1           1                214               0  ...          0          0  BENIGN\n",
              "2           2                214               0  ...          0          0  BENIGN\n",
              "3           3                179               0  ...          0          0  BENIGN\n",
              "4           4                213               0  ...          0          0  BENIGN\n",
              "5           5                213               0  ...          0          0  BENIGN\n",
              "6           6                213               0  ...          0          0  BENIGN\n",
              "7           7                213               0  ...          0          0  BENIGN\n",
              "8           8                 36               0  ...          0          0  BENIGN\n",
              "9           9                214               0  ...          0          0  BENIGN\n",
              "\n",
              "[10 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqdxZbMNZHr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c31da9-e335-4a05-8e31-3b7332a5b056"
      },
      "source": [
        "del df['Unnamed: 0']\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2827876 entries, 0 to 2830742\n",
            "Data columns (total 56 columns):\n",
            " #   Column                       Dtype \n",
            "---  ------                       ----- \n",
            " 0    Destination Port            uint8 \n",
            " 1    Flow Duration               uint8 \n",
            " 2    Total Fwd Packets           uint8 \n",
            " 3    Total Backward Packets      uint8 \n",
            " 4   Total Length of Fwd Packets  uint8 \n",
            " 5    Fwd Packet Length Max       uint8 \n",
            " 6    Fwd Packet Length Min       uint8 \n",
            " 7    Fwd Packet Length Mean      uint8 \n",
            " 8    Fwd Packet Length Std       uint8 \n",
            " 9   Bwd Packet Length Max        uint8 \n",
            " 10   Bwd Packet Length Min       uint8 \n",
            " 11   Bwd Packet Length Mean      uint8 \n",
            " 12   Bwd Packet Length Std       uint8 \n",
            " 13  Flow Bytes/s                 uint8 \n",
            " 14   Flow Packets/s              uint8 \n",
            " 15   Flow IAT Mean               uint8 \n",
            " 16   Flow IAT Std                uint8 \n",
            " 17   Flow IAT Max                uint8 \n",
            " 18   Flow IAT Min                uint8 \n",
            " 19  Fwd IAT Total                uint8 \n",
            " 20   Fwd IAT Mean                uint8 \n",
            " 21   Fwd IAT Std                 uint8 \n",
            " 22   Fwd IAT Max                 uint8 \n",
            " 23   Fwd IAT Min                 uint8 \n",
            " 24  Bwd IAT Total                uint8 \n",
            " 25   Bwd IAT Max                 uint8 \n",
            " 26  Fwd PSH Flags                uint8 \n",
            " 27   Fwd Header Length           uint8 \n",
            " 28   Bwd Header Length           uint8 \n",
            " 29  Fwd Packets/s                uint8 \n",
            " 30   Min Packet Length           uint8 \n",
            " 31   Max Packet Length           uint8 \n",
            " 32   Packet Length Mean          uint8 \n",
            " 33   Packet Length Std           uint8 \n",
            " 34   Packet Length Variance      uint8 \n",
            " 35  FIN Flag Count               uint8 \n",
            " 36   PSH Flag Count              uint8 \n",
            " 37   ACK Flag Count              uint8 \n",
            " 38   URG Flag Count              uint8 \n",
            " 39   Average Packet Size         uint8 \n",
            " 40   Avg Fwd Segment Size        uint8 \n",
            " 41   Avg Bwd Segment Size        uint8 \n",
            " 42   Fwd Header Length.1         uint8 \n",
            " 43  Subflow Fwd Packets          uint8 \n",
            " 44   Subflow Fwd Bytes           uint8 \n",
            " 45   Subflow Bwd Packets         uint8 \n",
            " 46   Subflow Bwd Bytes           uint8 \n",
            " 47  Init_Win_bytes_forward       uint8 \n",
            " 48   Init_Win_bytes_backward     uint8 \n",
            " 49   act_data_pkt_fwd            uint8 \n",
            " 50   min_seg_size_forward        uint8 \n",
            " 51  Idle Mean                    uint8 \n",
            " 52   Idle Std                    uint8 \n",
            " 53   Idle Max                    uint8 \n",
            " 54   Idle Min                    uint8 \n",
            " 55   Label                       object\n",
            "dtypes: object(1), uint8(55)\n",
            "memory usage: 191.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQHHjnHNZHlY",
        "outputId": "819f2507-cecb-4cd5-cf91-08df2cb6b7e5"
      },
      "source": [
        "df[' Label'].value_counts()\n",
        "\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/all_data.h5'\n",
        "# df.to_hdf(filename, 'data', mode='w', format='table')\n",
        "# \n",
        "# f = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/all_data.h5'\n",
        "# df = pd.read_hdf(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        2271320\n",
              "DoS Hulk                       230124\n",
              "PortScan                       158804\n",
              "DDoS                           128025\n",
              "DoS GoldenEye                   10293\n",
              "FTP-Patator                      7935\n",
              "SSH-Patator                      5897\n",
              "DoS slowloris                    5796\n",
              "DoS Slowhttptest                 5499\n",
              "Bot                              1956\n",
              "Web Attack � Brute Force         1507\n",
              "Web Attack � XSS                  652\n",
              "Infiltration                       36\n",
              "Web Attack � Sql Injection         21\n",
              "Heartbleed                         11\n",
              "Name:  Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPJQ1lydW33",
        "outputId": "ba15923b-a9f9-4327-8209-03f332e4f6e1"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2827876, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "uaAWk9l1et7z",
        "outputId": "ca362870-a8f0-4697-af30-8b75e8fb74ad"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>214</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>179</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>48</td>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Destination Port   Flow Duration  ...   Idle Min   Label\n",
              "0                213               0  ...          0  BENIGN\n",
              "1                214               0  ...          0  BENIGN\n",
              "2                214               0  ...          0  BENIGN\n",
              "3                179               0  ...          0  BENIGN\n",
              "4                213               0  ...          0  BENIGN\n",
              "\n",
              "[5 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrAV_rmQv4QV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL8Mu2OkgvIr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5VRrb7tybBG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH86HSh2wLJi"
      },
      "source": [
        "nb_epoch = 200\n",
        "batch_size = 128\n",
        "# input_dim = df_train_0_x_rescaled.shape[1] #num of predictor variables, \n",
        "# encoding_dim = 32\n",
        "# hidden_dim = int(encoding_dim / 2)\n",
        "learning_rate = 1e-3\n",
        "input_layer = Input(shape=(55 ))\n",
        "\n",
        "def encoder(input_layer):\n",
        "   \n",
        "  encoder1 = Dense(70, activation=\"relu\")(input_layer)\n",
        "  encoder1 = BatchNormalization()(encoder1)\n",
        "\n",
        "  encoder2 = Dense(60, activation=\"relu\")(encoder1)\n",
        "  encoder2 = BatchNormalization()(encoder2)\n",
        "\n",
        "  encoder3 = Dense(50, activation=\"relu\")(encoder2)\n",
        "  encoder3 = BatchNormalization()(encoder3)\n",
        "\n",
        "  encoder4 = Dense(40, activation=\"relu\")(encoder3)\n",
        "  encoder4 = BatchNormalization()(encoder4)\n",
        "\n",
        "  encoder5 = Dense(30, activation=\"relu\")(encoder4)\n",
        "  encoder5 = BatchNormalization()(encoder5)\n",
        "\n",
        "  encoder6 = Dense(20, activation=\"relu\")(encoder5)\n",
        "  encoder6 = BatchNormalization()(encoder6)\n",
        "  \n",
        "  encoder7 = Dense(10, activation=\"relu\")(encoder6)\n",
        "  encoder7 = BatchNormalization()(encoder7)\n",
        "\n",
        "  return encoder7\n",
        "\n",
        "def decoder(encoder7):\n",
        "  decoder1 = Dense(20, activation=\"relu\")(encoder7)\n",
        "  decoder1 = BatchNormalization()(decoder1)\n",
        "\n",
        "  decoder2 = Dense(30, activation=\"relu\")(decoder1)\n",
        "  decoder2 = BatchNormalization()(decoder2)\n",
        "\n",
        "  decoder3 = Dense(40, activation=\"relu\")(decoder2)\n",
        "  decoder3 = BatchNormalization()(decoder3)\n",
        "\n",
        "  decoder4 = Dense(50, activation=\"relu\")(decoder3)\n",
        "  decoder4 = BatchNormalization()(decoder4)\n",
        "\n",
        "  decoder5 = Dense(60, activation=\"relu\")(decoder4)\n",
        "  decoder5 = BatchNormalization()(decoder5)\n",
        "\n",
        "  decoder6 = Dense(70, activation=\"relu\")(decoder5)\n",
        "  decoder6 = BatchNormalization()(decoder6)\n",
        "# decoder7 = Dense(input_dim, activation=\"linear\")(decoder6)\n",
        "\n",
        "# up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
        "#     decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "#     return decoded\n",
        "\n",
        "  decoder7 = Dense(55, activation=\"linear\")(decoder6)\n",
        "  decoder7 = BatchNormalization()(decoder7)\n",
        "  # autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "  # autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "  return decoder7\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2fMrn0jdZSQ",
        "outputId": "64192cf8-eeda-46c2-9d04-dba3bb20f25d"
      },
      "source": [
        "autoencoder = Model(input_layer, decoder(encoder(input_layer)))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n",
        "autoencoder.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 55)]              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 70)                3920      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 70)                280       \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 60)                4260      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 60)                240       \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 50)                3050      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 40)                2040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 40)                160       \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 30)                1230      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 30)                120       \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 20)                80        \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 20)                220       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 20)                80        \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 30)                630       \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 30)                120       \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 40)                1240      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 40)                160       \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 50)                2050      \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 60)                3060      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 60)                240       \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 70)                4270      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 70)                280       \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 55)                3905      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 55)                220       \n",
            "=================================================================\n",
            "Total params: 33,125\n",
            "Trainable params: 31,915\n",
            "Non-trainable params: 1,210\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COjtn0D713tQ"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras import regularizers"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7DkfOAwL17f"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScrJeLgGKv3j"
      },
      "source": [
        "import numpy as np\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# Dropping all the rows with nan values\n",
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4uEFMrw1PZS"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "target = df[' Label']\n",
        "le = LabelEncoder()\n",
        "target = le.fit_transform(target)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq1Tnvwz1Pb2"
      },
      "source": [
        "# X = X.apply(lambda col: (col.astype('float16')))\n",
        "X=df.iloc[:,:-1]\n",
        "X.info()\n",
        "X.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR--qhiIKdaa",
        "outputId": "465079e9-a55f-4e96-d95d-25687c784ced"
      },
      "source": [
        "X.shape, target.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2827876, 55), (2827876,))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbj772RdILV-"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHushD9ANzPj"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bEHl4y45g1Z"
      },
      "source": [
        "# train_dataset, temp_test_dataset = train_test_split(df, test_size = 0.30)\n",
        "# test_dataset, valid_dataset = train_test_split(temp_test_dataset, test_size = 0.5)\n",
        "# train_dataset.shape, temp_test_dataset.shape, test_dataset.shape, valid_dataset.shape\n",
        "# test_dataset.sample(10)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko9oRRTm6AKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60859b8-87b7-4a84-9b70-74108465c59b"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, target, test_size = 0.20, random_state = 42)\n",
        "print(x_train.shape,len(y_train),x_test.shape,len(y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2262300, 55) 2262300 (565576, 55) 565576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLy0IogRNSXG",
        "outputId": "f6aa0a54-778e-4c2c-98fe-5079937a0729"
      },
      "source": [
        "print(x_train.shape, x_test.shape)\n",
        "y_train.shape, y_test.shape \n",
        "# y_test[0][0]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2262300, 55) (565576, 55)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2262300,), (565576,))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgKckXDK6h0V"
      },
      "source": [
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J446RmmSDfS2",
        "outputId": "1870c8a7-6453-4e2b-8bd3-e4f1801a80ea"
      },
      "source": [
        "autoencoder_train = autoencoder.fit(x_train,y_train, \n",
        "                                    batch_size=batch_size,\n",
        "                                    epochs=nb_epoch,\n",
        "                                    verbose=1,\n",
        "                                    validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "17675/17675 [==============================] - 157s 5ms/step - loss: 0.9964 - val_loss: 0.5597\n",
            "Epoch 2/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.6344 - val_loss: 0.4458\n",
            "Epoch 3/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.6207 - val_loss: 0.4571\n",
            "Epoch 4/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.5606 - val_loss: 0.3380\n",
            "Epoch 5/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.5121 - val_loss: 0.6308\n",
            "Epoch 6/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4933 - val_loss: 0.3462\n",
            "Epoch 7/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4845 - val_loss: 0.3375\n",
            "Epoch 8/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4703 - val_loss: 0.3228\n",
            "Epoch 9/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4584 - val_loss: 0.2915\n",
            "Epoch 10/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4547 - val_loss: 0.3124\n",
            "Epoch 11/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4511 - val_loss: 0.3835\n",
            "Epoch 12/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4436 - val_loss: 0.2859\n",
            "Epoch 13/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4422 - val_loss: 0.5633\n",
            "Epoch 14/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4357 - val_loss: 0.2627\n",
            "Epoch 15/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4420 - val_loss: 1.1621\n",
            "Epoch 16/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.6368 - val_loss: 0.6909\n",
            "Epoch 17/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.5112 - val_loss: 0.3479\n",
            "Epoch 18/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4757 - val_loss: 0.6201\n",
            "Epoch 19/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4499 - val_loss: 1.4262\n",
            "Epoch 20/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4605 - val_loss: 5.1438\n",
            "Epoch 21/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4317 - val_loss: 0.3135\n",
            "Epoch 22/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4314 - val_loss: 0.6718\n",
            "Epoch 23/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4286 - val_loss: 0.3881\n",
            "Epoch 24/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4233 - val_loss: 0.2676\n",
            "Epoch 25/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4167 - val_loss: 1.2821\n",
            "Epoch 26/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4157 - val_loss: 0.2730\n",
            "Epoch 27/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4206 - val_loss: 0.2714\n",
            "Epoch 28/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4144 - val_loss: 0.2477\n",
            "Epoch 29/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4157 - val_loss: 0.7656\n",
            "Epoch 30/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4150 - val_loss: 8.7310\n",
            "Epoch 31/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4107 - val_loss: 292.9070\n",
            "Epoch 32/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4095 - val_loss: 3.1392\n",
            "Epoch 33/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4059 - val_loss: 0.2552\n",
            "Epoch 34/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4095 - val_loss: 2.4882\n",
            "Epoch 35/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4138 - val_loss: 0.2433\n",
            "Epoch 36/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4100 - val_loss: 2.3884\n",
            "Epoch 37/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4054 - val_loss: 0.2852\n",
            "Epoch 38/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4159 - val_loss: 1.3479\n",
            "Epoch 39/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4133 - val_loss: 0.2410\n",
            "Epoch 40/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4103 - val_loss: 0.2576\n",
            "Epoch 41/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4173 - val_loss: 0.3523\n",
            "Epoch 42/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4122 - val_loss: 0.4160\n",
            "Epoch 43/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4085 - val_loss: 0.2877\n",
            "Epoch 44/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4127 - val_loss: 0.2493\n",
            "Epoch 45/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4087 - val_loss: 0.2760\n",
            "Epoch 46/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4058 - val_loss: 3.7081\n",
            "Epoch 47/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4048 - val_loss: 0.4154\n",
            "Epoch 48/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4082 - val_loss: 0.2546\n",
            "Epoch 49/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4054 - val_loss: 0.2711\n",
            "Epoch 50/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4126 - val_loss: 0.3618\n",
            "Epoch 51/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4103 - val_loss: 892.1803\n",
            "Epoch 52/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4179 - val_loss: 0.5632\n",
            "Epoch 53/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4273 - val_loss: 678.3508\n",
            "Epoch 54/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4241 - val_loss: 1538.7500\n",
            "Epoch 55/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4245 - val_loss: 25114.8574\n",
            "Epoch 56/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4201 - val_loss: 34.5164\n",
            "Epoch 57/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4213 - val_loss: 1852.4084\n",
            "Epoch 58/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4208 - val_loss: 0.9802\n",
            "Epoch 59/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4167 - val_loss: 46.7444\n",
            "Epoch 60/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4211 - val_loss: 127.9238\n",
            "Epoch 61/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4234 - val_loss: 3.2860\n",
            "Epoch 62/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4229 - val_loss: 1.2336\n",
            "Epoch 63/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4189 - val_loss: 0.5264\n",
            "Epoch 64/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4146 - val_loss: 3.7980\n",
            "Epoch 65/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4202 - val_loss: 0.2783\n",
            "Epoch 66/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4263 - val_loss: 15.2723\n",
            "Epoch 67/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4189 - val_loss: 5.5304\n",
            "Epoch 68/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4235 - val_loss: 0.2952\n",
            "Epoch 69/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4183 - val_loss: 0.4335\n",
            "Epoch 70/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4190 - val_loss: 1.3411\n",
            "Epoch 71/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4181 - val_loss: 2.0721\n",
            "Epoch 72/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4157 - val_loss: 1.2822\n",
            "Epoch 73/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4158 - val_loss: 1.2421\n",
            "Epoch 74/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4168 - val_loss: 2.6168\n",
            "Epoch 75/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4117 - val_loss: 11.9268\n",
            "Epoch 76/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4164 - val_loss: 3.5718\n",
            "Epoch 77/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4140 - val_loss: 34.0018\n",
            "Epoch 78/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4107 - val_loss: 37.6781\n",
            "Epoch 79/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4141 - val_loss: 1.5264\n",
            "Epoch 80/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4088 - val_loss: 1.4740\n",
            "Epoch 81/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4095 - val_loss: 13.2203\n",
            "Epoch 82/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4043 - val_loss: 0.5236\n",
            "Epoch 83/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4047 - val_loss: 10.0279\n",
            "Epoch 84/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4034 - val_loss: 0.2480\n",
            "Epoch 85/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4075 - val_loss: 1.7338\n",
            "Epoch 86/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4046 - val_loss: 0.2879\n",
            "Epoch 87/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4092 - val_loss: 0.3226\n",
            "Epoch 88/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4092 - val_loss: 0.3266\n",
            "Epoch 89/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4092 - val_loss: 0.3370\n",
            "Epoch 90/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4085 - val_loss: 0.2723\n",
            "Epoch 91/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4098 - val_loss: 0.3005\n",
            "Epoch 92/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4096 - val_loss: 2.9378\n",
            "Epoch 93/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4031 - val_loss: 184.3175\n",
            "Epoch 94/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4057 - val_loss: 4.2954\n",
            "Epoch 95/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4064 - val_loss: 567.8251\n",
            "Epoch 96/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.3999 - val_loss: 2887.9922\n",
            "Epoch 97/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4120 - val_loss: 32596.0488\n",
            "Epoch 98/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4082 - val_loss: 294.2298\n",
            "Epoch 99/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4102 - val_loss: 15.3175\n",
            "Epoch 100/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.4081 - val_loss: 315131.5625\n",
            "Epoch 101/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4077 - val_loss: 7795708.0000\n",
            "Epoch 102/200\n",
            "17675/17675 [==============================] - 88s 5ms/step - loss: 0.3962 - val_loss: 16.1838\n",
            "Epoch 103/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4156 - val_loss: 3673.3494\n",
            "Epoch 104/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4164 - val_loss: 47.9888\n",
            "Epoch 105/200\n",
            "17675/17675 [==============================] - 87s 5ms/step - loss: 0.4078 - val_loss: 77.3595\n",
            "Epoch 106/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4139 - val_loss: 921.9586\n",
            "Epoch 107/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4128 - val_loss: 670.2389\n",
            "Epoch 108/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4117 - val_loss: 40963.8438\n",
            "Epoch 109/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4138 - val_loss: 75.0975\n",
            "Epoch 110/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4160 - val_loss: 1.2836\n",
            "Epoch 111/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4133 - val_loss: 19242.0664\n",
            "Epoch 112/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4143 - val_loss: 23236.1582\n",
            "Epoch 113/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4138 - val_loss: 2.4995\n",
            "Epoch 114/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4179 - val_loss: 1.3609\n",
            "Epoch 115/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4201 - val_loss: 8.8588\n",
            "Epoch 116/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4166 - val_loss: 105.3124\n",
            "Epoch 117/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4147 - val_loss: 329.5588\n",
            "Epoch 118/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4136 - val_loss: 126418.2969\n",
            "Epoch 119/200\n",
            "17675/17675 [==============================] - 89s 5ms/step - loss: 0.4206 - val_loss: 30.2134\n",
            "Epoch 120/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4166 - val_loss: 98.4656\n",
            "Epoch 121/200\n",
            "17675/17675 [==============================] - 99s 6ms/step - loss: 0.4130 - val_loss: 9.1692\n",
            "Epoch 122/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4182 - val_loss: 3402.4719\n",
            "Epoch 123/200\n",
            "17675/17675 [==============================] - 99s 6ms/step - loss: 0.4166 - val_loss: 21291.3301\n",
            "Epoch 124/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4154 - val_loss: 12515089408.0000\n",
            "Epoch 125/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4104 - val_loss: 2385586683904.0000\n",
            "Epoch 126/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4156 - val_loss: 52385624.0000\n",
            "Epoch 127/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4190 - val_loss: 96455.7891\n",
            "Epoch 128/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4205 - val_loss: 10982.6084\n",
            "Epoch 129/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4139 - val_loss: 42406342656.0000\n",
            "Epoch 130/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4175 - val_loss: 8668948.0000\n",
            "Epoch 131/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4143 - val_loss: 152611405824.0000\n",
            "Epoch 132/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4227 - val_loss: 1910562816.0000\n",
            "Epoch 133/200\n",
            "17675/17675 [==============================] - 99s 6ms/step - loss: 0.4115 - val_loss: 88132.4609\n",
            "Epoch 134/200\n",
            "17675/17675 [==============================] - 98s 6ms/step - loss: 0.4132 - val_loss: 21640056.0000\n",
            "Epoch 135/200\n",
            "17675/17675 [==============================] - 98s 6ms/step - loss: 0.4176 - val_loss: 99084.9375\n",
            "Epoch 136/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4195 - val_loss: 37264461824.0000\n",
            "Epoch 137/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4112 - val_loss: 164437.7188\n",
            "Epoch 138/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4112 - val_loss: 16827.8574\n",
            "Epoch 139/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4051 - val_loss: 147.0661\n",
            "Epoch 140/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.3956 - val_loss: 21943.4082\n",
            "Epoch 141/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.3978 - val_loss: 0.3677\n",
            "Epoch 142/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.3953 - val_loss: 0.2876\n",
            "Epoch 143/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4034 - val_loss: 9.9994\n",
            "Epoch 144/200\n",
            "17675/17675 [==============================] - 99s 6ms/step - loss: 0.4100 - val_loss: 1.6996\n",
            "Epoch 145/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4013 - val_loss: 38.3603\n",
            "Epoch 146/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.3964 - val_loss: 5.7238\n",
            "Epoch 147/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.3918 - val_loss: 5.4399\n",
            "Epoch 148/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4026 - val_loss: 1.6681\n",
            "Epoch 149/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4019 - val_loss: 29.2495\n",
            "Epoch 150/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4106 - val_loss: 72.0721\n",
            "Epoch 151/200\n",
            "17675/17675 [==============================] - 102s 6ms/step - loss: 0.4079 - val_loss: 0.4789\n",
            "Epoch 152/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4042 - val_loss: 0.3614\n",
            "Epoch 153/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4088 - val_loss: 0.4376\n",
            "Epoch 154/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4131 - val_loss: 0.4055\n",
            "Epoch 155/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4083 - val_loss: 4.0841\n",
            "Epoch 156/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4105 - val_loss: 540.7638\n",
            "Epoch 157/200\n",
            "17675/17675 [==============================] - 98s 6ms/step - loss: 0.4066 - val_loss: 1938.6361\n",
            "Epoch 158/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4065 - val_loss: 250.9851\n",
            "Epoch 159/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4097 - val_loss: 5743.4604\n",
            "Epoch 160/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4046 - val_loss: 901.7750\n",
            "Epoch 161/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4035 - val_loss: 19652.3672\n",
            "Epoch 162/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4034 - val_loss: 116.4883\n",
            "Epoch 163/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4075 - val_loss: 476.5714\n",
            "Epoch 164/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.3974 - val_loss: 0.3556\n",
            "Epoch 165/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4023 - val_loss: 9.1971\n",
            "Epoch 166/200\n",
            "17675/17675 [==============================] - 100s 6ms/step - loss: 0.4046 - val_loss: 0.5196\n",
            "Epoch 167/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4094 - val_loss: 0.2407\n",
            "Epoch 168/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4010 - val_loss: 0.2478\n",
            "Epoch 169/200\n",
            "17675/17675 [==============================] - 101s 6ms/step - loss: 0.4027 - val_loss: 0.5031\n",
            "Epoch 170/200\n",
            "17675/17675 [==============================] - 99s 6ms/step - loss: 0.4013 - val_loss: 0.2482\n",
            "Epoch 171/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4102 - val_loss: 0.2473\n",
            "Epoch 172/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4002 - val_loss: 0.2575\n",
            "Epoch 173/200\n",
            "17675/17675 [==============================] - 98s 6ms/step - loss: 0.4080 - val_loss: 0.2589\n",
            "Epoch 174/200\n",
            "17675/17675 [==============================] - 97s 6ms/step - loss: 0.4077 - val_loss: 0.3759\n",
            "Epoch 175/200\n",
            "17675/17675 [==============================] - 98s 6ms/step - loss: 0.4105 - val_loss: 0.2709\n",
            "Epoch 176/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4116 - val_loss: 0.3400\n",
            "Epoch 177/200\n",
            "17675/17675 [==============================] - 102s 6ms/step - loss: 0.4109 - val_loss: 0.5621\n",
            "Epoch 178/200\n",
            "17675/17675 [==============================] - 90s 5ms/step - loss: 0.4156 - val_loss: 0.3131\n",
            "Epoch 179/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4095 - val_loss: 18.4820\n",
            "Epoch 180/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4117 - val_loss: 0.9592\n",
            "Epoch 181/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4097 - val_loss: 0.4019\n",
            "Epoch 182/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4078 - val_loss: 0.2996\n",
            "Epoch 183/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4111 - val_loss: 0.3269\n",
            "Epoch 184/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4135 - val_loss: 6.7494\n",
            "Epoch 185/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4105 - val_loss: 0.3349\n",
            "Epoch 186/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4059 - val_loss: 0.2782\n",
            "Epoch 187/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4082 - val_loss: 0.2771\n",
            "Epoch 188/200\n",
            "17675/17675 [==============================] - 91s 5ms/step - loss: 0.4054 - val_loss: 0.2870\n",
            "Epoch 189/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4032 - val_loss: 0.2466\n",
            "Epoch 190/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4056 - val_loss: 3.4948\n",
            "Epoch 191/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4075 - val_loss: 1.6851\n",
            "Epoch 192/200\n",
            "17675/17675 [==============================] - 94s 5ms/step - loss: 0.4047 - val_loss: 17.7370\n",
            "Epoch 193/200\n",
            "17675/17675 [==============================] - 93s 5ms/step - loss: 0.4042 - val_loss: 6.5830\n",
            "Epoch 194/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4054 - val_loss: 2.6677\n",
            "Epoch 195/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4045 - val_loss: 3.1488\n",
            "Epoch 196/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4063 - val_loss: 1.4764\n",
            "Epoch 197/200\n",
            "17675/17675 [==============================] - 97s 5ms/step - loss: 0.4087 - val_loss: 0.3670\n",
            "Epoch 198/200\n",
            "17675/17675 [==============================] - 92s 5ms/step - loss: 0.4080 - val_loss: 3.4619\n",
            "Epoch 199/200\n",
            "17675/17675 [==============================] - 96s 5ms/step - loss: 0.4091 - val_loss: 0.4080\n",
            "Epoch 200/200\n",
            "17675/17675 [==============================] - 95s 5ms/step - loss: 0.4042 - val_loss: 0.2489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-deLdtc0Sw1"
      },
      "source": [
        "# autoencoder.compile(metrics=['accuracy'],\n",
        "#                     loss='mean_squared_error',\n",
        "#                     optimizer='adam')\n",
        "# cp = ModelCheckpoint(filepath=\"MNAD-autoencoder_classifier.h5\",\n",
        "#                                save_best_only=True,\n",
        "#                                verbose=0)\n",
        "\n",
        "# tb = TensorBoard(log_dir='./logs',\n",
        "#                 histogram_freq=0,\n",
        "#                 write_graph=True,\n",
        "#                 write_images=True)\n",
        "                               \n",
        "# history = autoencoder.fit(train_dataset,\n",
        "# epochs=10,\n",
        "# batch_size=batch_size,\n",
        "# shuffle=True,\n",
        "# validation_data=(test_dataset),\n",
        "# verbose=1,\n",
        "# callbacks=[cp, tb]).history"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be32Rzlkk3sx"
      },
      "source": [
        "autoencoder.save_weights('/content/drive/MyDrive/7th- Semester/FashionMNIST/1MNADautoencoder.h5')\n",
        "# '/content/drive/MyDrive/7th- Semester/FashionMNIST/t10k-labels-idx1-ubyte.gz',10000)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "C3bPsbQNLbnK",
        "outputId": "d2525086-43c6-4aba-9f6a-c28c81631040"
      },
      "source": [
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(200)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfAV8A7csegE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLd77gZseih"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, target, test_size = 0.20, random_state = 42)\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIeGdJwRsels"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical # Change the labels from categorical to one-hot encoding\n",
        "train_Y_one_hot = to_categorical(y_train) \n",
        "test_Y_one_hot = to_categorical(y_test)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13CjlO8KsBYP"
      },
      "source": [
        "nb_epoch = 200\n",
        "batch_size = 128\n",
        "# input_dim = df_train_0_x_rescaled.shape[1] #num of predictor variables, \n",
        "# encoding_dim = 32\n",
        "# hidden_dim = int(encoding_dim / 2)\n",
        "learning_rate = 1e-3\n",
        "input_layer = Input(shape=(55 ))\n",
        "\n",
        "def encoder(input_layer):\n",
        "   \n",
        "  encoder1 = Dense(70, activation=\"relu\")(input_layer)\n",
        "  encoder1 = BatchNormalization()(encoder1)\n",
        "\n",
        "  encoder2 = Dense(60, activation=\"relu\")(encoder1)\n",
        "  encoder2 = BatchNormalization()(encoder2)\n",
        "\n",
        "  encoder3 = Dense(50, activation=\"relu\")(encoder2)\n",
        "  encoder3 = BatchNormalization()(encoder3)\n",
        "\n",
        "  encoder4 = Dense(40, activation=\"relu\")(encoder3)\n",
        "  encoder4 = BatchNormalization()(encoder4)\n",
        "\n",
        "  encoder5 = Dense(30, activation=\"relu\")(encoder4)\n",
        "  encoder5 = BatchNormalization()(encoder5)\n",
        "\n",
        "  encoder6 = Dense(20, activation=\"relu\")(encoder5)\n",
        "  encoder6 = BatchNormalization()(encoder6)\n",
        "  \n",
        "  encoder7 = Dense(10, activation=\"relu\")(encoder6)\n",
        "  encoder7 = BatchNormalization()(encoder7)\n",
        "\n",
        "  return encoder7"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrQS-jle4Bql",
        "outputId": "98d59468-4d51-4592-ca58-ce045835a085"
      },
      "source": [
        "full_model.layers"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7fedf7b50750>,\n",
              " <keras.layers.core.Dense at 0x7fedf76704d0>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf755e210>,\n",
              " <keras.layers.core.Dense at 0x7fedf7513f90>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf760a350>,\n",
              " <keras.layers.core.Dense at 0x7fedf7b38f10>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf75df650>,\n",
              " <keras.layers.core.Dense at 0x7fedf74d7e10>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf7674d90>,\n",
              " <keras.layers.core.Dense at 0x7fedf7498710>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf74e8f90>,\n",
              " <keras.layers.core.Dense at 0x7fedf74dca10>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf7436d50>,\n",
              " <keras.layers.core.Dense at 0x7fedf742c350>,\n",
              " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7fedf7480490>,\n",
              " <keras.layers.core.Flatten at 0x7fedf758a290>,\n",
              " <keras.layers.core.Dense at 0x7fedf744f650>,\n",
              " <keras.layers.core.Dense at 0x7fedf7456790>]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReY7-Bh4sBVG"
      },
      "source": [
        "def fc(enco):\n",
        "    flat = Flatten()(enco)\n",
        "    den = Dense(128, activation='relu')(flat)\n",
        "    out = Dense(15, activation='softmax')(den)\n",
        "    return out\n",
        "\n",
        "encode = encoder(input_layer)\n",
        "full_model = Model(input_layer,fc(encode))\n",
        "\n",
        "for l1,l2 in zip(full_model.layers[:10],autoencoder.layers[0:10]):\n",
        "    l1.set_weights(l2.get_weights())"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMQehuZZsBSR",
        "outputId": "7a2edec0-5214-4227-a922-27f8dc59c0e4"
      },
      "source": [
        "autoencoder.get_weights()[0][1]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.91767633e-01, -1.42938876e+00, -9.15879250e+00,  5.02667725e-01,\n",
              "       -4.59136754e-01, -1.91909909e-01, -3.70410293e-01, -1.09591089e-01,\n",
              "       -2.03493074e-01, -2.19275799e+01, -5.29524684e-02,  1.75487959e+00,\n",
              "       -4.85659152e-01, -1.31464386e+00,  5.48846066e-01,  1.99271679e-01,\n",
              "       -9.31533948e-02, -5.27369916e-01,  5.29194176e-01, -2.02974129e+00,\n",
              "       -5.58402419e-01,  8.96083415e-02, -2.84534264e+00,  4.11202610e-01,\n",
              "       -8.17105830e-01,  7.07773149e-01, -4.06331234e-02,  3.07630658e-01,\n",
              "       -3.66916806e-02, -1.18307486e-01,  1.21080017e+00,  7.13586360e-02,\n",
              "       -1.90082192e-03, -1.11489487e+00, -2.50084907e-01, -4.15610909e-01,\n",
              "        9.93418768e-02, -3.02211738e+00, -1.64441913e-01,  4.39830031e-03,\n",
              "       -1.40933961e-01, -6.28853291e-02, -4.96179074e-01, -8.62860531e-02,\n",
              "       -2.34327853e-01, -1.27282189e-02, -1.17022134e-02,  1.47465670e+00,\n",
              "       -2.02432126e-01, -6.04608804e-02, -2.13749096e-01, -1.12860680e+00,\n",
              "        1.04193187e+00,  6.10765934e-01, -1.14191923e+01, -1.27226263e-01,\n",
              "       -1.07923043e+00, -5.34715280e-02, -8.56532604e-02, -7.37183988e-01,\n",
              "        6.26758635e-02, -2.97257274e-01, -3.21332359e+00,  7.04067171e-01,\n",
              "        1.40319586e-01,  2.14385375e-01,  4.69147749e-02, -3.79636437e-02,\n",
              "       -1.80200294e-01, -9.34939235e-02], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7srBltSsBPW",
        "outputId": "485c34a5-0321-494c-83b2-1f53301ce561"
      },
      "source": [
        "full_model.get_weights()[0][1]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.91767633e-01, -1.42938876e+00, -9.15879250e+00,  5.02667725e-01,\n",
              "       -4.59136754e-01, -1.91909909e-01, -3.70410293e-01, -1.09591089e-01,\n",
              "       -2.03493074e-01, -2.19275799e+01, -5.29524684e-02,  1.75487959e+00,\n",
              "       -4.85659152e-01, -1.31464386e+00,  5.48846066e-01,  1.99271679e-01,\n",
              "       -9.31533948e-02, -5.27369916e-01,  5.29194176e-01, -2.02974129e+00,\n",
              "       -5.58402419e-01,  8.96083415e-02, -2.84534264e+00,  4.11202610e-01,\n",
              "       -8.17105830e-01,  7.07773149e-01, -4.06331234e-02,  3.07630658e-01,\n",
              "       -3.66916806e-02, -1.18307486e-01,  1.21080017e+00,  7.13586360e-02,\n",
              "       -1.90082192e-03, -1.11489487e+00, -2.50084907e-01, -4.15610909e-01,\n",
              "        9.93418768e-02, -3.02211738e+00, -1.64441913e-01,  4.39830031e-03,\n",
              "       -1.40933961e-01, -6.28853291e-02, -4.96179074e-01, -8.62860531e-02,\n",
              "       -2.34327853e-01, -1.27282189e-02, -1.17022134e-02,  1.47465670e+00,\n",
              "       -2.02432126e-01, -6.04608804e-02, -2.13749096e-01, -1.12860680e+00,\n",
              "        1.04193187e+00,  6.10765934e-01, -1.14191923e+01, -1.27226263e-01,\n",
              "       -1.07923043e+00, -5.34715280e-02, -8.56532604e-02, -7.37183988e-01,\n",
              "        6.26758635e-02, -2.97257274e-01, -3.21332359e+00,  7.04067171e-01,\n",
              "        1.40319586e-01,  2.14385375e-01,  4.69147749e-02, -3.79636437e-02,\n",
              "       -1.80200294e-01, -9.34939235e-02], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yTqYo91vS9l"
      },
      "source": [
        "for layer in full_model.layers[0:10]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGJQkkzcsBM2"
      },
      "source": [
        "full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(), metrics=['mse','mae', 'accuracy'])\n",
        "# loss='mean_squared_error', optimizer = RMSprop()\n",
        "\n",
        "# loss='mse', optimizer='adam', metrics=['mse', 'mae', 'mape', 'cosine', 'Accuracy']"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG7dlJqbsBJ7"
      },
      "source": [
        "full_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsTe4aj77DQH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr1f6p3E7DTv",
        "outputId": "4f791014-8647-4db7-cacb-7b7e065de725"
      },
      "source": [
        "# Change the labels from categorical to one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical # Change the labels from categorical to one-hot encoding\n",
        "train_Y_one_hot = to_categorical(y_train) \n",
        "test_Y_one_hot = to_categorical(y_test)\n",
        "\n",
        "# Display the change for category label using one-hot encoding\n",
        "print('Original label:', y_train[0])\n",
        "print('After conversion to one-hot:', train_Y_one_hot[0])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label: 0\n",
            "After conversion to one-hot: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGokQ4tu7jA0"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "tf.data.experimental.enable_debug_mode()"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riuD3Ih-sBGx",
        "outputId": "5492f599-6866-4a65-cbea-97116017a4b0"
      },
      "source": [
        "# classify_train = full_model.fit(x_train, y_train, batch_size=64,epochs=50,verbose=1,validation_data=(x_test, y_test))\n",
        "classify_train = full_model.fit(x_train,train_Y_one_hot, \n",
        "                                    batch_size=batch_size,\n",
        "                                    epochs=50,\n",
        "                                    verbose=1,\n",
        "                                    validation_data=(x_test, test_Y_one_hot))\n",
        "\n",
        "# classify_train = full_model.fit(train_X, train_label, batch_size=64,epochs=50,verbose=1,validation_data=(valid_X, valid_label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 2891/17675 [===>..........................] - ETA: 7:41 - loss: 0.0173 - mse: 5.7308e-04 - mae: 0.0011 - accuracy: 0.9943"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNkWHVSVsBDx",
        "outputId": "67af71f9-eee7-49e3-b1f1-3a450bc098cf"
      },
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "x_test.shape, y_test.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2262300, 55) (2262300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((565576, 55), (565576,))"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-aR81KsBAr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX94EQxksA76"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX4YqDEbsA40"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfa78eWjsAz1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}